{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Numbeo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv,fnmatch,os,re,pickle\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import StratifiedKFold,cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy import interp\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from time import time\n",
    "from numbeo_data_acquisition import *\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Numbeo : loading serialized files or downloading data according to the location or the region / country from numbeo API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = '/home/thebatou/Documents/Uni_Potsdam/Competitive_Data_Analysis/data/'\n",
    "file_name = 'project_data_train_v2.csv'\n",
    "file_location = os.path.join(data_folder, file_name)\n",
    "\n",
    "print 'Trying to load bitbond data ...'\n",
    "bitbond_data = pd.read_csv(file_location, index_col=0)\n",
    "bitbond_data.set_value([835, 1232, 1839, 1993], 'region', 'NA')\n",
    "bitbond_data.set_value([1364, 1366, 1374, 1425, 1459, 1580, 1753], 'currency', 'USD')\n",
    "bitbond_data.set_value([1362, 1368], 'currency', 'HUF')\n",
    "bitbond_data.set_value([1360, 1416, 1525, 1921], 'currency', 'IDR')\n",
    "bitbond_data.set_value([1384, 1428, 1463, 1553, 1574, 1628, 1808], 'currency', 'INR')\n",
    "bitbond_data.set_value([1369, 1435, 1693], 'currency', 'KES')\n",
    "print 'Done loading bitbond data! {} data were loaded.'.format(bitbond_data.shape[0])\n",
    "\n",
    "try:\n",
    "    print (\"Trying to load numbeo data about region ...\")\n",
    "    file_location1 = os.path.join(data_folder, 'numbeo_region_prices.csv')\n",
    "    region_prices_numbeo = pd.read_csv(file_location1, index_col=0)\n",
    "    print \"Done loading region numbeo data! {} data were loaded.\".format(region_prices_numbeo.shape[0])    \n",
    "\n",
    "    print (\"Trying to load numbeo data about country ...\")\n",
    "    file_location2 = os.path.join(data_folder, 'numbeo_country_prices.csv')\n",
    "    country_prices_numbeo = pd.read_csv(file_location2, index_col=0)\n",
    "    print \"Done loading country numbeo data! {} data were loaded.\".format(country_prices_numbeo.shape[0])\n",
    "    \n",
    "    print (\"Trying to load numbeo data about city ...\")\n",
    "    file_location3 = os.path.join(data_folder, 'numbeo_city_prices.csv')\n",
    "    city_prices_numbeo = pd.read_csv(file_location3, index_col=0)\n",
    "    print \"Done loading city numbeo data! {} data were loaded.\".format(city_prices_numbeo.shape[0])\n",
    "    \n",
    "    \n",
    "except IOError as e:\n",
    "    print \"I/O error: {0}\".format(e)\n",
    "    print \"Downloading data from numbeo website ...\"\n",
    "    quadruple_id_lat_long_region = bitbond_data.groupby(by=('loan_identifier','address_lat','address_lng','region')).indices.keys()\n",
    "    (all_city_prices_requests, all_country_prices_requests, missing_prices_location) =  get_numbeo_data_according_to_location(quadruple_id_lat_long_region)\n",
    "    print 'Missing value : {}'.format(len(missing_prices_location) != 0)\n",
    "    \n",
    "    city_prices_numbeo = pd.DataFrame(all_city_prices_requests)\n",
    "    city_prices_numbeo['prices'] = average_salary_extraction(city_prices_numbeo['prices'])\n",
    "    city_prices_numbeo.to_csv(path_or_buf=file_location3, encoding='utf-8')\n",
    "    \n",
    "    country_prices_numbeo = pd.DataFrame(all_country_prices_requests)\n",
    "    country_prices_numbeo['prices'] = average_salary_extraction(country_prices_numbeo['prices'])\n",
    "    country_prices_numbeo.to_csv(path_or_buf=file_location2, encoding='utf-8')\n",
    "    \n",
    "    regions = np.unique(bitbond_data.region)\n",
    "    (all_region_prices_requests, missing_prices_regions) = get_numbeo_data_according_to_region(regions)\n",
    "    print 'Missing value : {}'.format(len(missing_prices_regions) != 0)\n",
    "\n",
    "    region_prices_numbeo = pd.DataFrame(all_region_prices_requests)\n",
    "    region_prices_numbeo['prices'] = average_salary_extraction(region_prices_numbeo['prices'])\n",
    "    region_prices_numbeo.to_csv(path_or_buf=file_location1, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Bitbond data shape : {}'.format(bitbond_data.shape)\n",
    "print 'City numbeo data has shape : {}'.format(city_prices_numbeo.shape)\n",
    "print 'Country numbeo data has shape : {}'.format(country_prices_numbeo.shape)\n",
    "print '\\nFirst element of city numbeo data :'\n",
    "print city_prices_numbeo.ix[0]\n",
    "print '\\nFirst element of country numbeo data :'\n",
    "print country_prices_numbeo.ix[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning bitbond data + Droping some useless data collected from Numbeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bitbond_data = bitbond_data.replace(to_replace=np.nan, value=-1)\n",
    "\n",
    "def drop_useless_column(dataframe):\n",
    "    dataframe.drop('currency',axis=1,inplace=True)\n",
    "    dataframe.drop('contributors',axis=1,inplace=True)\n",
    "    dataframe.drop('monthLastUpdate',axis=1,inplace=True)\n",
    "    dataframe.drop('yearLastUpdate',axis=1,inplace=True)\n",
    "    dataframe = dataframe.rename(columns={'name':'location'})\n",
    "    return dataframe\n",
    "\n",
    "city_prices_numbeo = drop_useless_column(city_prices_numbeo)\n",
    "country_prices_numbeo = drop_useless_column(country_prices_numbeo)\n",
    "region_prices_numbeo = drop_useless_column(region_prices_numbeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Nombre de pair (lat, lon) unique : {}\".format(len(bitbond_data.groupby(by=('address_lat','address_lng')).indices.keys()))\n",
    "print \"Nombre d'emprunteur : {}\".format(len(np.unique(bitbond_data['borrower_identifier'])))\n",
    "\n",
    "l1 = bitbond_data.groupby(by=('address_lat','address_lng','borrower_identifier')).indices.keys()\n",
    "l2 = [(e[0],e[1]) for e in l1]\n",
    "borrowers_without_address = [e[2] for e in l1 if l2.count((e[0],e[1])) > 1]\n",
    "# We substract 1 because of the pair (-1,-1) in the unique addresses (pair)\n",
    "print \"Nombre d'emprunteur sans adresses : {}\".format(len(borrowers_without_address))\n",
    "\n",
    "#bitbond_data.loc[bitbond_data['borrower_identifier'].isin(borrowers_without_address)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the bitbond data with the numbeo data to make one unique dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bitbond_numbeo_data = pd.merge(bitbond_data, region_prices_numbeo, on=['region'], how='left')\n",
    "bitbond_numbeo_data_2 = pd.merge(bitbond_numbeo_data, country_prices_numbeo, on=['loan_identifier','region','prices', 'location'], how='left')\n",
    "data = pd.merge(bitbond_numbeo_data_2, city_prices_numbeo, on=['loan_identifier','address_lat', 'address_lng', 'prices', 'location'], how='left')\n",
    "data.to_csv(path_or_buf=os.path.join(data_folder,'data_num_bit.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv,fnmatch,os,re,pickle\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import StratifiedKFold,cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy import interp\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from time import time\n",
    "from numbeo_data_acquisition import *\n",
    "from feature_engineering import *\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the data ...\n",
      "Done loading the data! (2177, 30) data were loaded.\n"
     ]
    }
   ],
   "source": [
    "file_loc = '/home/thebatou/Documents/Uni_Potsdam/Competitive_Data_Analysis/data/data_num_bit.csv'\n",
    "print (\"Trying to load the data ...\")\n",
    "data = pd.read_csv(file_loc, index_col=0)\n",
    "print \"Done loading the data! {} data were loaded.\".format(data.shape)    \n",
    "data = data.set_value([835, 1232, 1839, 1993], 'region', 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data[~data['location'].isin(np.unique(data['location'])[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Nominal interest rate : {}'.format(np.unique(data.nominal_interest_rate))\n",
    "print 'Term : {}'.format(np.unique(data.term))\n",
    "print 'Purpose : {}'.format(np.unique(data.purpose))\n",
    "print 'Borrower rating : {}'.format(np.unique(data.borrower_rating))\n",
    "print 'Employment : {}'.format(np.unique(data.employment))\n",
    "print 'Region : {}'.format(np.unique(data.region))\n",
    "print 'Prices : {}'.format(np.unique(data.prices))\n",
    "print 'Facebook : {}'.format(np.unique(data.facebook))\n",
    "print 'Ebay : {}'.format(np.unique(data.ebay))\n",
    "print 'Status : {}'.format(np.unique(data.status))\n",
    "print 'Fraudulent : {}'.format(np.unique(data.fraudulent))\n",
    "print 'Currency : {}'.format(np.unique(data.currency))\n",
    "print 'Rates count : {}'.format(np.unique(data.rates_count))\n",
    "print 'Rates paid : {}'.format(np.unique(data.rates_paid))\n",
    "print 'Base currency : {}'.format(np.unique(data.base_currency))\n",
    "print 'Location : {}'.format(np.unique(data.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation of the data : building the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['average_salary'] = data['net_income_cents'] / data['prices']\n",
    "data['average_salary'] = pd.DataFrame(preprocessing.scale(data['average_salary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classif_data = data.loc[data['status'].isin(['defaulted', 'fully_paid', 'late_90', 'charged_off'])]\n",
    "classif_data = classif_data.copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data : (608, 32)\n"
     ]
    }
   ],
   "source": [
    "print 'Shape of the data : {}'.format(classif_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = classif_data['status'] == 'fully_paid'\n",
    "y = y.as_matrix()\n",
    "classif_data.drop('status',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classif_data.drop('loan_identifier',axis=1,inplace=True)\n",
    "classif_data.drop('nominal_interest_rate',axis=1,inplace=True)\n",
    "classif_data.drop('borrower_identifier',axis=1,inplace=True)\n",
    "classif_data.drop('borrower_rating',axis=1,inplace=True)\n",
    "classif_data.drop('region',axis=1,inplace=True)\n",
    "classif_data.drop('facebook',axis=1,inplace=True)\n",
    "classif_data.drop('twitter',axis=1,inplace=True)\n",
    "classif_data.drop('paypal',axis=1,inplace=True)\n",
    "classif_data.drop('ebay',axis=1,inplace=True)\n",
    "classif_data.drop('linkedin',axis=1,inplace=True)\n",
    "classif_data.drop('published_at',axis=1,inplace=True)\n",
    "classif_data.drop('funded_at',axis=1,inplace=True)\n",
    "classif_data.drop('amount_requested',axis=1,inplace=True)\n",
    "classif_data.drop('amount_funded',axis=1,inplace=True)\n",
    "classif_data.drop('issuer_id',axis=1,inplace=True)\n",
    "classif_data.drop('fraudulent',axis=1,inplace=True)\n",
    "classif_data.drop('address_lat',axis=1,inplace=True)\n",
    "classif_data.drop('address_lng',axis=1,inplace=True)\n",
    "classif_data.drop('net_income_cents',axis=1,inplace=True)\n",
    "classif_data.drop('currency',axis=1,inplace=True)\n",
    "classif_data.drop('base_currency',axis=1,inplace=True)\n",
    "classif_data.drop('rates_count',axis=1,inplace=True)\n",
    "classif_data.drop('rates_paid',axis=1,inplace=True)\n",
    "classif_data.drop('location',axis=1,inplace=True)\n",
    "classif_data.drop('prices',axis=1,inplace=True)\n",
    "#classif_data.drop('project_description',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                                  0\n",
       "term                                                        term_6_weeks\n",
       "purpose                                                       investment\n",
       "project_description    I need to make some investments but need to co...\n",
       "employment                                                 self_employed\n",
       "average_salary                                               -0.08705384\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_data.ix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.624s.\n"
     ]
    }
   ],
   "source": [
    "categorical_feature = ['term', 'purpose', 'employment']\n",
    "other_feature = [column for column in classif_data.columns.tolist() if column not in categorical_feature]\n",
    "\n",
    "encoders, feat = build_categorical_feature(classif_data,categorical_feature)\n",
    "encoded_data = encode_data(feat.loc[:,categorical_feature])\n",
    "\n",
    "encoded_df = pd.DataFrame(data=encoded_data.toarray(), columns=['term','term','term','term','purpose','purpose','purpose','purpose',\n",
    "                                                   'purpose','purpose','purpose','employment','employment','employment'\n",
    "                                                  ,'employment','employment'])\n",
    "\n",
    "cl_data = pd.concat([feat.loc[:,other_feature],encoded_df],axis=1).copy()\n",
    "articles_topics = text_transformation(cl_data['project_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similar(doc_id, doc_topic_distribution):\n",
    "    doc_rep = np.array([doc_topic_distribution[doc_id]]*len(doc_topic_distribution))\n",
    "    sim = np.dot(((np.log(doc_rep)-np.log(doc_topic_distribution))),doc_topic_distribution[doc_id])\n",
    "    return sim.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([131,  16, 328, 127, 132,  72, 133,  95, 182,  23])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_topics_norm = articles_topics/articles_topics.sum(axis=1)[:,None]\n",
    "similar_docs = get_similar(1,docs_topics_norm)\n",
    "similar_docs[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my third reputation loan. Of course this one will be repaid on time as the others.\n",
      "I need short term liquidity. I run an eBay shop with sports gear and orders are coming in. The loan will help me to buy the goods right away so I can fulfill the orders immediately.\n"
     ]
    }
   ],
   "source": [
    "print cl_data.project_description.ix[1]\n",
    "print cl_data.project_description.ix[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = data['status'] != 'fully_paid'\n",
    "#v = DictVectorizer()\n",
    "#D = data.loc[:,['amount_funded','amount_requested']].T.to_dict().values()\n",
    "#.to_dict()#,'address_lat','address_lng' , 'net_income_cents'\n",
    "                #'nominal_interest_rate', 'prices', 'rates_count', 'rates_paid']]\n",
    "#X = v.fit_transform(D)\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "cv = StratifiedKFold(y, n_folds=6)\n",
    "clf = svm.SVC(kernel='linear', probability=True,\n",
    "                     random_state=random_state)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "scores = []\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i, (train, test) in tqdm(enumerate(cv)):\n",
    "    clf.fit(X[train], y[train])\n",
    "    scores.append(clf.score(X[test], y[test]))\n",
    "    y_scores = clf.decision_function(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], y_scores)\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    # Now predict the value of the digit on the second half:\n",
    "    expected = y[test]\n",
    "    predicted = clf.predict(X[test])\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (clf, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "mean_tpr /= len(cv)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = X.todense()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "train_samples = 100  # Samples used for training the models\n",
    "\n",
    "X_train = X[:train_samples]\n",
    "X_test = X[train_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_test = y[train_samples:]\n",
    "\n",
    "# Create classifiers\n",
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "svc = LinearSVC(C=1.0)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Plot calibration plots\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "for clf, name in [(lr, 'Logistic'),\n",
    "                  (gnb, 'Naive Bayes'),\n",
    "                  (svc, 'Support Vector Classification'),\n",
    "                  (rfc, 'Random Forest')]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "    else:  # use decision function\n",
    "        prob_pos = clf.decision_function(X_test)\n",
    "        prob_pos = \\\n",
    "            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    fraction_of_positives, mean_predicted_value = \\\n",
    "        calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "             label=\"%s\" % (name, ))\n",
    "\n",
    "    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "             histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "rnd = check_random_state(1)\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "# l1 data (only 5 informative features)\n",
    "X_1, y_1 = X, y\n",
    "\n",
    "# l2 data\n",
    "X_2, y_2 = X, y\n",
    "\n",
    "clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=1e-3),\n",
    "             np.logspace(-2.3, 1.3, 10), X_1, y_1),\n",
    "            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=1e-4),\n",
    "             np.logspace(-4.5, 2, 10), X_2, y_2)]\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c']\n",
    "\n",
    "for fignum, (clf, cs, X, y) in tqdm(enumerate(clf_sets)):\n",
    "    # set up the plot for each regressor\n",
    "    plt.figure(fignum, figsize=(9, 10))\n",
    "\n",
    "    for k, train_size in tqdm(enumerate(np.linspace(0.3, 0.7, 3)[::-1])):\n",
    "        param_grid = dict(C=cs)\n",
    "        # To get nice curve, we need a large number of iterations to\n",
    "        # reduce the variance\n",
    "        grid = GridSearchCV(clf, refit=False, param_grid=param_grid,\n",
    "                            cv=ShuffleSplit(n=n_samples, train_size=train_size,\n",
    "                                            n_iter=250, random_state=1))\n",
    "        grid.fit(X, y)\n",
    "        scores = [x[1] for x in grid.grid_scores_]\n",
    "\n",
    "        scales = [(1, 'No scaling'),\n",
    "                  ((n_samples * train_size), '1/n_samples'),\n",
    "                  ]\n",
    "\n",
    "        for subplotnum, (scaler, name) in enumerate(scales):\n",
    "            plt.subplot(2, 1, subplotnum + 1)\n",
    "            plt.xlabel('C')\n",
    "            plt.ylabel('CV Score')\n",
    "            grid_cs = cs * float(scaler)  # scale the C's\n",
    "            plt.semilogx(grid_cs, scores, label=\"fraction %.2f\" %\n",
    "                         train_size)\n",
    "            plt.title('scaling=%s, penalty=%s, loss=%s' %\n",
    "                      (name, clf.penalty, clf.loss))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "#random_state = np.random.RandomState(0)\n",
    "\n",
    "cv = StratifiedKFold(y, n_folds=6)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "#clf = svm.SVC(probability=True)\n",
    "\n",
    "cross_val_score(clf, X, y, cv=10)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "scores = []\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i, (train, test) in tqdm(enumerate(cv)):\n",
    "    clf.fit(X[train], y[train])\n",
    "    scores.append(clf.score(X[test], y[test]))\n",
    "    y_scores = clf.predict(X[test]) #decision_function(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], y_scores)\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    expected = y[test]\n",
    "    predicted = clf.predict(X[test])\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (clf, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "mean_tpr /= len(cv)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
